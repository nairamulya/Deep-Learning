{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Forest Cover classification with BatchNormalization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPByogEbRpDUIBOvv0KeWam",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nairamulya/Deep-Learning/blob/master/Forest_Cover_classification_with_BatchNormalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th5B4rwI_lie",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd9a656d-acb6-4cb5-cf58-61c1ab1ea126"
      },
      "source": [
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import fetch_covtype\n",
        "\n",
        "import tensorflow as tf \n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "\n",
        "tf.set_random_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "X,y=fetch_covtype(return_X_y=True)\n",
        "print(type(X)) \n",
        "print(X.shape) \n",
        "print(y.shape) \n",
        "X_10 = X[:,:10]\n",
        "print(X_10.shape) \n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X10_train, X10_test, y10_train, y10_test = train_test_split(X_10, y, test_size=0.1,stratify=y, random_state=42)\n",
        "\n",
        "\n",
        "print(X10_test.shape) \n",
        "X = X10_test \n",
        "y = y10_test\n",
        "print(set(y))\n",
        "y = y-1 \n",
        "print(set(y)) \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y, random_state=42)\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "sc = StandardScaler()\n",
        "\n",
        "\n",
        "sc.fit(X_train)\n",
        "X_train_std = sc.transform(X_train) \n",
        "X_test_std = sc.transform(X_test)\n",
        "\n",
        "model_DNN = keras.models.Sequential()\n",
        "model_DNN.add(keras.layers.Dense(units=25, activation='relu', input_shape= X_train.shape[1:]))\n",
        "model_DNN.add(keras.layers.BatchNormalization())\n",
        "model_DNN.add(keras.layers.Dense(units=20, activation='relu'))\n",
        "model_DNN.add(keras.layers.Dense(units=15, activation='relu'))\n",
        "model_DNN.add(keras.layers.Dense(units=10, activation='relu'))\n",
        "model_DNN.add(keras.layers.Dense(units=7, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model_DNN.summary()\n",
        "\n",
        "model_DNN.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_DNN.fit(x=X_train_std, y=y_train, validation_split=0.1, epochs=50, batch_size=16)\n",
        " \n",
        "\n",
        "test_loss, test_accuracy = model_DNN.evaluate(x=X_test_std, y=y_test) \n",
        "\n",
        "print(test_loss, test_accuracy)\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://ndownloader.figshare.com/files/5976039\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(581012, 54)\n",
            "(581012,)\n",
            "(581012, 10)\n",
            "(58102, 10)\n",
            "{1, 2, 3, 4, 5, 6, 7}\n",
            "{0, 1, 2, 3, 4, 5, 6}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 25)                275       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 25)                100       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                520       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 15)                315       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                160       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 7)                 77        \n",
            "=================================================================\n",
            "Total params: 1,447\n",
            "Trainable params: 1,397\n",
            "Non-trainable params: 50\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 41832 samples, validate on 4649 samples\n",
            "Epoch 1/50\n",
            "41832/41832 [==============================] - 6s 153us/sample - loss: 0.8439 - acc: 0.6308 - val_loss: 0.6795 - val_acc: 0.7105\n",
            "Epoch 2/50\n",
            "41832/41832 [==============================] - 5s 130us/sample - loss: 0.7300 - acc: 0.6866 - val_loss: 0.6704 - val_acc: 0.7051\n",
            "Epoch 3/50\n",
            "41832/41832 [==============================] - 5s 128us/sample - loss: 0.7106 - acc: 0.6905 - val_loss: 0.6375 - val_acc: 0.7240\n",
            "Epoch 4/50\n",
            "41832/41832 [==============================] - 5s 127us/sample - loss: 0.6964 - acc: 0.6980 - val_loss: 0.6354 - val_acc: 0.7186\n",
            "Epoch 5/50\n",
            "41832/41832 [==============================] - 5s 128us/sample - loss: 0.6866 - acc: 0.7003 - val_loss: 0.6244 - val_acc: 0.7275\n",
            "Epoch 6/50\n",
            "41832/41832 [==============================] - 5s 128us/sample - loss: 0.6788 - acc: 0.7027 - val_loss: 0.6224 - val_acc: 0.7296\n",
            "Epoch 7/50\n",
            "41832/41832 [==============================] - 5s 124us/sample - loss: 0.6750 - acc: 0.7035 - val_loss: 0.6104 - val_acc: 0.7316\n",
            "Epoch 8/50\n",
            "41832/41832 [==============================] - 5s 128us/sample - loss: 0.6658 - acc: 0.7089 - val_loss: 0.6068 - val_acc: 0.7371\n",
            "Epoch 9/50\n",
            "41832/41832 [==============================] - 5s 131us/sample - loss: 0.6610 - acc: 0.7113 - val_loss: 0.6062 - val_acc: 0.7311\n",
            "Epoch 10/50\n",
            "41832/41832 [==============================] - 5s 131us/sample - loss: 0.6610 - acc: 0.7108 - val_loss: 0.6044 - val_acc: 0.7356\n",
            "Epoch 11/50\n",
            "41832/41832 [==============================] - 5s 129us/sample - loss: 0.6587 - acc: 0.7114 - val_loss: 0.6048 - val_acc: 0.7324\n",
            "Epoch 12/50\n",
            "41832/41832 [==============================] - 5s 131us/sample - loss: 0.6535 - acc: 0.7131 - val_loss: 0.5940 - val_acc: 0.7391\n",
            "Epoch 13/50\n",
            "41832/41832 [==============================] - 5s 130us/sample - loss: 0.6538 - acc: 0.7142 - val_loss: 0.6001 - val_acc: 0.7406\n",
            "Epoch 14/50\n",
            "41832/41832 [==============================] - 5s 128us/sample - loss: 0.6480 - acc: 0.7155 - val_loss: 0.5973 - val_acc: 0.7412\n",
            "Epoch 15/50\n",
            "41832/41832 [==============================] - 5s 129us/sample - loss: 0.6446 - acc: 0.7181 - val_loss: 0.5983 - val_acc: 0.7354\n",
            "Epoch 16/50\n",
            "41832/41832 [==============================] - 5s 131us/sample - loss: 0.6462 - acc: 0.7178 - val_loss: 0.5919 - val_acc: 0.7380\n",
            "Epoch 17/50\n",
            "41832/41832 [==============================] - 6s 134us/sample - loss: 0.6446 - acc: 0.7179 - val_loss: 0.5872 - val_acc: 0.7393\n",
            "Epoch 18/50\n",
            "41832/41832 [==============================] - 6s 134us/sample - loss: 0.6441 - acc: 0.7202 - val_loss: 0.5904 - val_acc: 0.7477\n",
            "Epoch 19/50\n",
            "41832/41832 [==============================] - 6s 134us/sample - loss: 0.6394 - acc: 0.7213 - val_loss: 0.5819 - val_acc: 0.7451\n",
            "Epoch 20/50\n",
            "41832/41832 [==============================] - 6s 136us/sample - loss: 0.6351 - acc: 0.7217 - val_loss: 0.5786 - val_acc: 0.7436\n",
            "Epoch 21/50\n",
            "41832/41832 [==============================] - 9s 213us/sample - loss: 0.6373 - acc: 0.7213 - val_loss: 0.5874 - val_acc: 0.7421\n",
            "Epoch 22/50\n",
            "41832/41832 [==============================] - 5s 131us/sample - loss: 0.6343 - acc: 0.7211 - val_loss: 0.5730 - val_acc: 0.7479\n",
            "Epoch 23/50\n",
            "41832/41832 [==============================] - 6s 132us/sample - loss: 0.6350 - acc: 0.7227 - val_loss: 0.5802 - val_acc: 0.7505\n",
            "Epoch 24/50\n",
            "41832/41832 [==============================] - 5s 124us/sample - loss: 0.6345 - acc: 0.7243 - val_loss: 0.5827 - val_acc: 0.7462\n",
            "Epoch 25/50\n",
            "41832/41832 [==============================] - 5s 129us/sample - loss: 0.6301 - acc: 0.7255 - val_loss: 0.5766 - val_acc: 0.7483\n",
            "Epoch 26/50\n",
            "41832/41832 [==============================] - 5s 126us/sample - loss: 0.6284 - acc: 0.7256 - val_loss: 0.5828 - val_acc: 0.7458\n",
            "Epoch 27/50\n",
            "41832/41832 [==============================] - 5s 128us/sample - loss: 0.6295 - acc: 0.7259 - val_loss: 0.5670 - val_acc: 0.7509\n",
            "Epoch 28/50\n",
            "41832/41832 [==============================] - 5s 124us/sample - loss: 0.6322 - acc: 0.7244 - val_loss: 0.5806 - val_acc: 0.7501\n",
            "Epoch 29/50\n",
            "41832/41832 [==============================] - 5s 127us/sample - loss: 0.6267 - acc: 0.7271 - val_loss: 0.5753 - val_acc: 0.7524\n",
            "Epoch 30/50\n",
            "41832/41832 [==============================] - 5s 125us/sample - loss: 0.6247 - acc: 0.7273 - val_loss: 0.5765 - val_acc: 0.7529\n",
            "Epoch 31/50\n",
            "41832/41832 [==============================] - 5s 127us/sample - loss: 0.6281 - acc: 0.7256 - val_loss: 0.5769 - val_acc: 0.7531\n",
            "Epoch 32/50\n",
            "41832/41832 [==============================] - 5s 123us/sample - loss: 0.6273 - acc: 0.7275 - val_loss: 0.5715 - val_acc: 0.7494\n",
            "Epoch 33/50\n",
            "41832/41832 [==============================] - 5s 125us/sample - loss: 0.6219 - acc: 0.7275 - val_loss: 0.5809 - val_acc: 0.7490\n",
            "Epoch 34/50\n",
            "41832/41832 [==============================] - 5s 125us/sample - loss: 0.6246 - acc: 0.7288 - val_loss: 0.5662 - val_acc: 0.7531\n",
            "Epoch 35/50\n",
            "41832/41832 [==============================] - 5s 125us/sample - loss: 0.6239 - acc: 0.7291 - val_loss: 0.5680 - val_acc: 0.7522\n",
            "Epoch 36/50\n",
            "41832/41832 [==============================] - 5s 124us/sample - loss: 0.6237 - acc: 0.7297 - val_loss: 0.5692 - val_acc: 0.7524\n",
            "Epoch 37/50\n",
            "41832/41832 [==============================] - 5s 124us/sample - loss: 0.6250 - acc: 0.7274 - val_loss: 0.5736 - val_acc: 0.7541\n",
            "Epoch 38/50\n",
            "41832/41832 [==============================] - 5s 126us/sample - loss: 0.6239 - acc: 0.7274 - val_loss: 0.5673 - val_acc: 0.7458\n",
            "Epoch 39/50\n",
            "41832/41832 [==============================] - 5s 124us/sample - loss: 0.6213 - acc: 0.7279 - val_loss: 0.5708 - val_acc: 0.7477\n",
            "Epoch 40/50\n",
            "41832/41832 [==============================] - 5s 123us/sample - loss: 0.6243 - acc: 0.7263 - val_loss: 0.5690 - val_acc: 0.7513\n",
            "Epoch 41/50\n",
            "41832/41832 [==============================] - 5s 126us/sample - loss: 0.6221 - acc: 0.7283 - val_loss: 0.5634 - val_acc: 0.7541\n",
            "Epoch 42/50\n",
            "41832/41832 [==============================] - 6s 141us/sample - loss: 0.6206 - acc: 0.7285 - val_loss: 0.5753 - val_acc: 0.7475\n",
            "Epoch 43/50\n",
            "41832/41832 [==============================] - 5s 122us/sample - loss: 0.6188 - acc: 0.7288 - val_loss: 0.5684 - val_acc: 0.7505\n",
            "Epoch 44/50\n",
            "41832/41832 [==============================] - 5s 125us/sample - loss: 0.6170 - acc: 0.7305 - val_loss: 0.5737 - val_acc: 0.7509\n",
            "Epoch 45/50\n",
            "41832/41832 [==============================] - 5s 124us/sample - loss: 0.6195 - acc: 0.7297 - val_loss: 0.5686 - val_acc: 0.7533\n",
            "Epoch 46/50\n",
            "41832/41832 [==============================] - 5s 126us/sample - loss: 0.6189 - acc: 0.7292 - val_loss: 0.5631 - val_acc: 0.7496\n",
            "Epoch 47/50\n",
            "41832/41832 [==============================] - 5s 122us/sample - loss: 0.6191 - acc: 0.7306 - val_loss: 0.5665 - val_acc: 0.7541\n",
            "Epoch 48/50\n",
            "41832/41832 [==============================] - 5s 124us/sample - loss: 0.6181 - acc: 0.7299 - val_loss: 0.5649 - val_acc: 0.7584\n",
            "Epoch 49/50\n",
            "41832/41832 [==============================] - 5s 124us/sample - loss: 0.6164 - acc: 0.7306 - val_loss: 0.5629 - val_acc: 0.7546\n",
            "Epoch 50/50\n",
            "41832/41832 [==============================] - 5s 124us/sample - loss: 0.6175 - acc: 0.7307 - val_loss: 0.5722 - val_acc: 0.7481\n",
            "11621/11621 [==============================] - 0s 26us/sample - loss: 0.5917 - acc: 0.7362\n",
            "0.5916571448432139 0.7361673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x4Lvth8_98U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}